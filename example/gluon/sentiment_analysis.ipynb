{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis with pretrained language model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load gluon\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Load SA raw data using gluon.data\n",
    "train = gluon.data.text.IMDB(root='data/imdb', segment='train')\n",
    "test = gluon.data.text.IMDB(root='data/imdb', segment='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Load user-defined tokenizer and Tokenize SA raw data\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "train_tokenized, train_labels = [tokenizer(text), score for text, score in train]\n",
    "test_tokenized, test_labels = [tokenizer(text), score for text, score in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Map tokenized data into nd array based instances according to lm's training data vocab\n",
    "from mxnet.gluon import text #issue here\n",
    "def get_frequencies(dataset):\n",
    "    return collections.Counter(x for tup in dataset for x in tup[0]+tup[1][-1:])\n",
    "lm_train_dataset = gluon.data.text.WikiText2(segment='train')\n",
    "vocab = text.vocab.Vocabulary(get_frequencies(lm_train_dataset))\n",
    "\n",
    "def encode_sentences(x_raw_samples, vocab):\n",
    "    #TODO\n",
    "    return x_encoded_samples\n",
    "    \n",
    "def encode_labels(y_raw_samples):\n",
    "    #TODO\n",
    "    return y_encoded_samples\n",
    "\n",
    "def pad_sample(x_encoded_samples, maxlen = 500, val = 0):\n",
    "    #TODO\n",
    "    return x_samples\n",
    "    \n",
    "x_encoded_train = encode_sentences(train_tokenized, vocab)\n",
    "x_encoded_test = encode_sentences(test_tokenized, vocab)\n",
    "\n",
    "x_train = mx.nd.array(pad_sample(x_encoded_train, 500, 0))\n",
    "x_test = mx.nd.array(pad_sample(x_encoded_test, 500, 0))\n",
    "\n",
    "y_train = mx.nd.array(encode_labels(train_labels))\n",
    "y_test = mx.nd.array(encode_labels(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Build SA classifier: pretrained lm encoder's hidden state as feature + binary dense layer as decoder\n",
    "from mxnet.gluon.model_zoo.text.lm import AWDLSTM\n",
    "\n",
    "class SALSTM(AWDLSTM):\n",
    "    def __init__(self, mode, vocab, embed_dim, hidden_dim, num_layers,\n",
    "                 dropout=0.5, drop_h=0.5, drop_i=0.5, drop_e=0.1, weight_drop=0,\n",
    "                 tie_weights=False, nclass, **kwargs):\n",
    "        super(SALSTM, self).__init__(self, mode, vocab, embed_dim, hidden_dim, num_layers,\n",
    "                 dropout=0.5, drop_h=0.5, drop_i=0.5, drop_e=0.1, weight_drop=0,\n",
    "                 tie_weights=False, **kwargs)\n",
    "        self._mode = mode\n",
    "        self._embed_dim = embed_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        self._drop_h = drop_h\n",
    "        self._drop_i = drop_i\n",
    "        self._drop_e = drop_e\n",
    "        self._weight_drop = weight_drop\n",
    "        self._tie_weights = tie_weights\n",
    "        self.embedding = self._get_embedding()\n",
    "        self.encoder = self._get_encoder()\n",
    "        self.decoder = gluon.nn.Dense(nclass)\n",
    "        with self.name_scope():\n",
    "            self.add(self.embedding)\n",
    "            self.add(self.encoder)\n",
    "            self.add(self.decoder)\n",
    "        def begin_state(self, *args, **kwargs):\n",
    "            return self.encoder[0].begin_state(*args, **kwargs)\n",
    "        def forward(self, inputs, begin_state=None): # pylint: disable=arguments-differ\n",
    "            embedded_inputs = self.embedding(inputs)\n",
    "            if not begin_state:\n",
    "                begin_state = self.begin_state()\n",
    "            encoded, state = self.encoder(embedded_inputs, begin_state)\n",
    "            out = self.decoder(state)\n",
    "            return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Report evaluation results: train and test accuracy\n",
    "def eval(x_samples, y_samples):\n",
    "    #TODO\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Train SA model and evaluate on train and test set\n",
    "context = mx.gpu(0)\n",
    "\n",
    "##pretrained lm parameter set\n",
    "lm_mode = 'lstm'\n",
    "lm_emsize = 400\n",
    "lm_nhid = 1150\n",
    "lm_nlayers = 3\n",
    "lm_dropout = 0.4\n",
    "lm_dropout_h = 0.3\n",
    "lm_dropout_i = 0.4\n",
    "lm_dropout_e = 0.1\n",
    "lm_weight_dropout = 0.65\n",
    "lm_tied = True\n",
    "##SA parameter set\n",
    "nclass = 2\n",
    "##hyper parameters\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "##\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "model = SALSTM(lm_mode, vocab, lm_emsize, lm_nhid, lm_nlayers,\n",
    "                    lm_dropout, lm_dropout_h, lm_dropout_i, lm_dropout_e, lm_weight_dropout,\n",
    "                    lm_tied, nclass)\n",
    "\n",
    "model.initialize(mx.init.Xavier(), ctx = context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                       {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(x_train):\n",
    "        data = data.as_in_context(context)\n",
    "        target = y_train[i].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            L = loss(output, target)\n",
    "        L.backward()\n",
    "        trainer.step(batch_size)\n",
    "    train_accuracy = eval(x_train, y_train)\n",
    "    test_accuracy = eval(x_test, y_test)\n",
    "    print(\"Epoch %s. Train_acc %s, Test_acc %s\"%(epoch, train_accuracy, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
