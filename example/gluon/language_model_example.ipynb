{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import time\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import data, text\n",
    "from mxnet.gluon.model_zoo.text.lm import RNNModel, AWDLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load language model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO:\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MXNet Autograd RNN/LSTM Language Model on Wikitext-2.')\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='type of recurrent net (rnn_tanh, rnn_relu, lstm, gru)')\n",
    "parser.add_argument('--emsize', type=int, default=400,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=1150,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=3,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=30,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=750,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=80, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.4,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--dropout_h', type=float, default=0.3,\n",
    "                    help='dropout applied to hidden layer (0 = no dropout)')\n",
    "parser.add_argument('--dropout_i', type=float, default=0.4,\n",
    "                    help='dropout applied to input layer (0 = no dropout)')\n",
    "parser.add_argument('--dropout_e', type=float, default=0.1,\n",
    "                    help='dropout applied to embedding layer (0 = no dropout)')\n",
    "parser.add_argument('--weight_dropout', type=float, default=0.65,\n",
    "                    help='weight dropout applied to h2h weight matrix (0 = no weight dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save', type=str, default='model.params',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--gctype', type=str, default='none',\n",
    "                    help='type of gradient compression to use, \\\n",
    "                          takes `2bit` or `none` for now.')\n",
    "parser.add_argument('--gcthreshold', type=float, default=0.5,\n",
    "                    help='threshold for 2bit gradient compression')\n",
    "parser.add_argument('--eval_only', action='store_true',\n",
    "                    help='Whether to only evaluate the trained model')\n",
    "parser.add_argument('--gpus', type=str,\n",
    "                    help='list of gpus to run, e.g. 0 or 0,2,5. empty means using cpu. (the result of multi-gpu training might be slightly different compared to single-gpu training, still need to be finalized)')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load training, val, testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [mx.cpu()] if args.gpus is None or args.gpus == \"\" else \\\n",
    "          [mx.gpu(int(i)) for i in args.gpus.split(',')]\n",
    "\n",
    "train_dataset = data.text.WikiText2(segment='train', seq_len=args.bptt,\n",
    "                                    eos='<eos>')\n",
    "\n",
    "def get_frequencies(dataset):\n",
    "    return collections.Counter(x for tup in dataset for x in tup[0] if x)\n",
    "\n",
    "vocab = text.vocab.Vocabulary(get_frequencies(train_dataset))\n",
    "def index_tokens(data, label):\n",
    "    return vocab[data], vocab[label]\n",
    "\n",
    "val_dataset, test_dataset = [data.text.WikiText2(segment=segment,\n",
    "                                                 seq_len=args.bptt,\n",
    "                                                 eos='<eos>')\n",
    "                             for segment in ['val', 'test']]\n",
    "\n",
    "nbatch_train = len(train_dataset) // args.batch_size\n",
    "train_data = gluon.data.DataLoader(train_dataset.transform(index_tokens),\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   sampler=gluon.contrib.data.IntervalSampler(len(train_dataset),\n",
    "                                                                              nbatch_train),\n",
    "                                   last_batch='discard')\n",
    "\n",
    "nbatch_val = len(val_dataset) // args.batch_size\n",
    "val_data = gluon.data.DataLoader(val_dataset.transform(index_tokens),\n",
    "                                 batch_size=args.batch_size,\n",
    "                                 sampler=gluon.contrib.data.IntervalSampler(len(val_dataset),\n",
    "                                                                            nbatch_val),\n",
    "                                 last_batch='discard')\n",
    "\n",
    "nbatch_test = len(test_dataset) // args.batch_size\n",
    "test_data = gluon.data.DataLoader(test_dataset.transform(index_tokens),\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  sampler=gluon.contrib.data.IntervalSampler(len(test_dataset),\n",
    "                                                                             nbatch_test),\n",
    "                                  last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "\n",
    "if args.weight_dropout:\n",
    "    model = AWDLSTM(args.model, vocab, args.emsize, args.nhid, args.nlayers,\n",
    "                    args.dropout, args.dropout_h, args.dropout_i, args.dropout_e, args.weight_dropout,\n",
    "                    args.tied)\n",
    "else:\n",
    "    model = RNNModel(args.model, vocab, args.emsize, args.nhid,\n",
    "                     args.nlayers, args.dropout, args.tied)\n",
    "\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "\n",
    "compression_params = None if args.gctype == 'none' else {'type': args.gctype, 'threshold': args.gcthreshold}\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args.lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0},\n",
    "                        compression_params=compression_params)\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train and evaluate language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=args.batch_size, ctx=context[0])\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(context[0]).T\n",
    "        target= target.as_in_context(context[0]).T\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(mx.nd.reshape(output, (-3, -1)),\n",
    "                 mx.nd.reshape(target, (-1,)))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        hiddens = [model.begin_state(func=mx.nd.zeros, batch_size=args.batch_size, ctx=ctx) for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            start_batch_time = time.time()\n",
    "            data = data.T\n",
    "            target= target.T\n",
    "            data_list = gluon.utils.split_and_load(data, context, even_split=False)\n",
    "            target_list = gluon.utils.split_and_load(target, context, even_split=False)\n",
    "            hiddens = [detach(hidden) for hidden in hiddens]\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    Ls.append(loss(mx.nd.reshape(output, (-3, -1)), mx.nd.reshape(y, (-1,))))\n",
    "                    hiddens[j] = h\n",
    "            for L in Ls:\n",
    "                L.backward()\n",
    "            for ctx in context:\n",
    "                grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "                gluon.utils.clip_global_norm(grads, args.clip * args.bptt * args.batch_size)\n",
    "\n",
    "            trainer.step(args.batch_size)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(L).asscalar() for L in Ls])\n",
    "\n",
    "            if i % args.log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / args.bptt / args.batch_size / args.log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, ppl %.2f'%(\n",
    "                    epoch, i, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "            print('[Epoch %d Batch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, i, args.batch_size / (time.time() - start_batch_time)))\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, (args.batch_size * nbatch_train) / (time.time() - start_epoch_time)))\n",
    "        val_L = eval(val_data)\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = eval(test_data)\n",
    "            model.collect_params().save(args.save)\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (args.batch_size * nbatch_train * args.epochs) / (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and report language model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "model.collect_params().load(args.save, context)\n",
    "val_L = eval(val_data)\n",
    "test_L = eval(test_data)\n",
    "print('Best validation loss %.2f, test ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "print('Total time cost %.2fs'%(time.time()-start_pipeline_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
