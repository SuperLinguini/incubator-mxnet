{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import time\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import data, text\n",
    "from mxnet.gluon.model_zoo.text.lm import StandardRNN, AWDRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load language model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pretrained lm parameter set\n",
    "mode = 'lstm'\n",
    "emsize = 400\n",
    "nhid = 1150\n",
    "nlayers = 3\n",
    "dropout = 0.4\n",
    "dropout_h = 0.3\n",
    "dropout_i = 0.4\n",
    "dropout_e = 0.1\n",
    "weight_dropout = 0.65\n",
    "tied = True\n",
    "##SA parameter set\n",
    "nclass = 2\n",
    "##hyper parameters\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "bptt = 35\n",
    "##\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load training, val, testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(2)\n",
    "\n",
    "train_dataset = data.text.WikiText2(segment='train', seq_len=bptt, eos='<eos>')\n",
    "\n",
    "def get_frequencies(dataset):\n",
    "    return collections.Counter(x for tup in dataset for x in tup[0] if x)\n",
    "\n",
    "vocab = text.vocab.Vocabulary(get_frequencies(train_dataset), reserved_tokens=['<eos>', '<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./wikitext2_vocab.json') as file:\n",
    "    file.write(vocab.json_serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Interval 59306 must be smaller than length 59306",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-835a7cfbd2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    sampler=gluon.contrib.data.IntervalSampler(len(train_dataset),\n\u001b[0;32m---> 13\u001b[0;31m                                                                               nbatch_train),\n\u001b[0m\u001b[1;32m     14\u001b[0m                                    last_batch='discard')\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cgwang/incubator-mxnet/python/mxnet/gluon/contrib/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, length, interval, rollover)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;34m\"Interval {} must be smaller than length {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Interval 59306 must be smaller than length 59306"
     ]
    }
   ],
   "source": [
    "def index_tokens(data, label):\n",
    "    return vocab[data], vocab[label]\n",
    "\n",
    "val_dataset, test_dataset = [data.text.WikiText2(segment=segment,\n",
    "                                                 seq_len=bptt,\n",
    "                                                 eos='<eos>')\n",
    "                             for segment in ['val', 'test']]\n",
    "\n",
    "nbatch_train = len(train_dataset) // batch_size\n",
    "train_data = gluon.data.DataLoader(train_dataset.transform(index_tokens),\n",
    "                                   batch_size=batch_size,\n",
    "                                   sampler=gluon.contrib.data.IntervalSampler(len(train_dataset),\n",
    "                                                                              nbatch_train),\n",
    "                                   last_batch='discard')\n",
    "\n",
    "nbatch_val = len(val_dataset) // batch_size\n",
    "val_data = gluon.data.DataLoader(val_dataset.transform(index_tokens),\n",
    "                                 batch_size=batch_size,\n",
    "                                 sampler=gluon.contrib.data.IntervalSampler(len(val_dataset),\n",
    "                                                                            nbatch_val),\n",
    "                                 last_batch='discard')\n",
    "\n",
    "nbatch_test = len(test_dataset) // batch_size\n",
    "test_data = gluon.data.DataLoader(test_dataset.transform(index_tokens),\n",
    "                                  batch_size=batch_size,\n",
    "                                  sampler=gluon.contrib.data.IntervalSampler(len(test_dataset),\n",
    "                                                                             nbatch_test),\n",
    "                                  last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Interval 59306 must be smaller than length 59306",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ca1a4e13cb19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    sampler=gluon.contrib.data.IntervalSampler(len(train_dataset),\n\u001b[0;32m----> 7\u001b[0;31m                                                                               nbatch_train),\n\u001b[0m\u001b[1;32m      8\u001b[0m                                    last_batch='discard')\n",
      "\u001b[0;32m~/cgwang/incubator-mxnet/python/mxnet/gluon/contrib/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, length, interval, rollover)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;34m\"Interval {} must be smaller than length {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Interval 59306 must be smaller than length 59306"
     ]
    }
   ],
   "source": [
    "def index_tokens(data, label):\n",
    "    return vocab[data], vocab[label]\n",
    "nbatch_train = len(train_dataset) // batch_size\n",
    "train_data = gluon.data.DataLoader(train_dataset.transform(index_tokens),\n",
    "                                   batch_size=batch_size,\n",
    "                                   sampler=gluon.contrib.data.IntervalSampler(len(train_dataset),\n",
    "                                                                              nbatch_train),\n",
    "                                   last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)\n",
    "\n",
    "if args.weight_dropout:\n",
    "    model = AWDLSTM(mode, vocab, emsize, nhid, nlayers,\n",
    "                    dropout, dropout_h, dropout_i, dropout_e, weight_dropout,\n",
    "                    tied)\n",
    "else:\n",
    "    model = RNNModel(mode, vocab, emsize, nhid,\n",
    "                     nlayers, dropout, tied)\n",
    "\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "\n",
    "compression_params = None if args.gctype == 'none' else {'type': args.gctype, 'threshold': args.gcthreshold}\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args.lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0},\n",
    "                        compression_params=compression_params)\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train and evaluate language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=args.batch_size, ctx=context[0])\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(context[0]).T\n",
    "        target= target.as_in_context(context[0]).T\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(mx.nd.reshape(output, (-3, -1)),\n",
    "                 mx.nd.reshape(target, (-1,)))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(args.epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        hiddens = [model.begin_state(func=mx.nd.zeros, batch_size=args.batch_size, ctx=ctx) for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            start_batch_time = time.time()\n",
    "            data = data.T\n",
    "            target= target.T\n",
    "            data_list = gluon.utils.split_and_load(data, context, even_split=False)\n",
    "            target_list = gluon.utils.split_and_load(target, context, even_split=False)\n",
    "            hiddens = [detach(hidden) for hidden in hiddens]\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    Ls.append(loss(mx.nd.reshape(output, (-3, -1)), mx.nd.reshape(y, (-1,))))\n",
    "                    hiddens[j] = h\n",
    "            for L in Ls:\n",
    "                L.backward()\n",
    "            for ctx in context:\n",
    "                grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "                gluon.utils.clip_global_norm(grads, args.clip * args.bptt * args.batch_size)\n",
    "\n",
    "            trainer.step(args.batch_size)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(L).asscalar() for L in Ls])\n",
    "\n",
    "            if i % args.log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / args.bptt / args.batch_size / args.log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, ppl %.2f'%(\n",
    "                    epoch, i, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "            print('[Epoch %d Batch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, i, args.batch_size / (time.time() - start_batch_time)))\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, (args.batch_size * nbatch_train) / (time.time() - start_epoch_time)))\n",
    "        val_L = eval(val_data)\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = eval(test_data)\n",
    "            model.collect_params().save(args.save)\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (args.batch_size * nbatch_train * args.epochs) / (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and report language model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "model.collect_params().load(args.save, context)\n",
    "val_L = eval(val_data)\n",
    "test_L = eval(test_data)\n",
    "print('Best validation loss %.2f, test ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "print('Total time cost %.2fs'%(time.time()-start_pipeline_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
